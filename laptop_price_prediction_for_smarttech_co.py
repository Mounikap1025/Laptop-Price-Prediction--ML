# -*- coding: utf-8 -*-
"""Laptop Price Prediction for SmartTech Co.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zwhFyUQ8jUDu5GJhKExywoq9uWtnJZo0
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("laptop.csv")
df.head()

print("Shape of the dataset:", df.shape)

print("\nData types and non-null counts:")
print(df.info())

print("\nSummary statistics:")
print(df.describe())

print("\nMissing values per column:")
print(df.isnull().sum())

missing_values = df.isnull().sum()
print("Missing values in each column:\n", missing_values)

print(df.columns)

# Drop any columns that start with "Unnamed"
df = df.loc[:, ~df.columns.str.contains("^Unnamed")]

print("Cleaned column names:")
print(df.columns)

# Drop rows with any missing values
df = df.dropna()

# Confirm new shape
print("Shape after dropping missing values:", df.shape)

# Optional: Confirm no missing values left
print("Remaining missing values:\n", df.isnull().sum())

# Show unique values in the 'Inches' column
print(df['Inches'].unique())

# Keep only rows where 'Inches' is a valid number
df = df[df['Inches'].str.replace('.', '', 1).str.isnumeric()]

df['Inches'] = df['Inches'].astype(float)

# Remove non-numeric RAM values
df = df[df['Ram'].str.contains('GB')]
df['Ram'] = df['Ram'].str.replace('GB', '').astype(int)

# Remove non-numeric weight values
df = df[df['Weight'].str.contains('kg')]
df['Weight'] = df['Weight'].str.replace('kg', '').astype(float)

# Extract first word (e.g., Intel, AMD, etc.)
df['Cpu_Brand'] = df['Cpu'].apply(lambda x: x.split()[0])

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df['Price'], bins=30, kde=True, color='skyblue')

plt.title("Price Distribution")
plt.xlabel("Price")
plt.ylabel("Count")
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Apply log transformation to Price
df['Log_Price'] = np.log(df['Price'])

# Step 2: Plot the distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['Log_Price'], bins=30, kde=True, color='steelblue')

plt.title("Price Distribution")
plt.xlabel("Price")
plt.ylabel("Count")
plt.grid(True)
plt.tight_layout()
plt.show()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Log transform the price column if not done already
df['Log_Price'] = np.log(df['Price'])

# Plot box plot to visualize outliers
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['Log_Price'], color='steelblue')
plt.title("Price Distribution")
plt.xlabel("Price")
plt.grid(True)
plt.tight_layout()
plt.show()

# Calculate Q1 and Q3
Q1 = df['Log_Price'].quantile(0.25)
Q3 = df['Log_Price'].quantile(0.75)
IQR = Q3 - Q1

# Define bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out the outliers
df_no_outliers = df[(df['Log_Price'] >= lower_bound) & (df['Log_Price'] <= upper_bound)]

print(f"Original data size: {df.shape[0]}")
print(f"Data size after outlier removal: {df_no_outliers.shape[0]}")

import pandas as pd

# Select only numeric columns
numeric_df = df.select_dtypes(include=['int64', 'float64'])

# Compute correlation matrix
correlation_matrix = numeric_df.corr()

import matplotlib.pyplot as plt
import seaborn as sns

# Set figure size
plt.figure(figsize=(12, 8))

# Create heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

# Add title
plt.title("Correlation Heatmap of Numerical Features")

# Show plot
plt.show()

['Company', 'TypeName', 'Cpu_Brand', 'Gpu', 'OpSys']

print(df.columns.tolist())

df['Cpu_Brand'] = df['Cpu'].apply(lambda x: x.split()[0])

# Only encode the columns that actually exist in your DataFrame
df = pd.get_dummies(df, columns=['Cpu_Brand'], drop_first=True)

# Features and target
X = df.drop('Price', axis=1)
y = df['Price']

from sklearn.model_selection import train_test_split

# 80% train, 20% test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.select_dtypes(include='object').columns)

# Drop it from the entire dataset
df = df.drop('ScreenResolution', axis=1)

X = df.drop('Price', axis=1)
y = df['Price']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

df['Cpu_Brand'] = df['Cpu'].apply(lambda x: x.split()[0])

df = df.drop('Cpu', axis=1)

print(df.select_dtypes(include='object').columns)

cat_cols = df.select_dtypes(include='object').columns
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# Define features and target
X = df.drop('Price', axis=1)
y = df['Price']

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)

import numpy as np

print("Linear Regression RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

print("Linear Regression R²:", r2_score(y_test, y_pred_lr))
print("Linear Regression RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_lr)))

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest R²:", r2_score(y_test, y_pred_rf))
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

from sklearn.ensemble import GradientBoostingRegressor

gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)

print("Gradient Boosting R²:", r2_score(y_test, y_pred_gb))
print("Gradient Boosting RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_gb)))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}

grid = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, scoring='neg_mean_squared_error')
grid.fit(X_train, y_train)

print("Best parameters:", grid.best_params_)
best_rf = grid.best_estimator_

import matplotlib.pyplot as plt

importances = rf.feature_importances_
features = X.columns

indices = np.argsort(importances)[-10:]  # Top 10

plt.figure(figsize=(10,6))
plt.title("Top 10 Feature Importances")
plt.barh(range(len(indices)), importances[indices], align="center")
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

# 1. . Which features have the most significant impact on laptop prices?
import matplotlib.pyplot as plt
import seaborn as sns

importances = rf.feature_importances_
features = X.columns
indices = importances.argsort()[::-1]

plt.figure(figsize=(12, 6))
sns.barplot(x=importances[indices][:15], y=features[indices][:15])
plt.title("Top 15 Feature Importances")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

# 2. Can the model accurately predict the prices of laptops from lesser-known brands?
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

lesser_brands = ['Chuwi', 'Vero', 'Mediacom', 'Xiaomi']

for brand in lesser_brands:
    col_name = 'Company_' + brand
    if col_name in df.columns:
        subset = df[df[col_name] == 1]
        if not subset.empty:
            X_subset = subset.drop('Price', axis=1)
            y_subset = subset['Price']
            y_pred_subset = rf.predict(X_subset)

            print(f"\nBrand: {brand}")
            print("R² Score:", r2_score(y_subset, y_pred_subset))
            print("RMSE:", np.sqrt(mean_squared_error(y_subset, y_pred_subset)))
        else:
            print(f"\nBrand: {brand} has no matching rows in dataset.")
    else:
        print(f"\nBrand: {brand} not found in encoded DataFrame.")

# 3. Does the brand of the laptop significantly influence its price?
import pandas as pd

# Extract only company dummy columns
company_cols = [col for col in df.columns if col.startswith('Company_')]

company_means = {}

for col in company_cols:
    company_name = col.split('_')[1]
    mean_price = df[df[col] == 1]['Price'].mean()
    company_means[company_name] = mean_price

# Convert to series and sort
company_means = pd.Series(company_means).sort_values(ascending=False)
print(company_means)

# 3. Does the brand of the laptop significantly influence its price?
import pandas as pd
import matplotlib.pyplot as plt

# Extract only company dummy columns
company_cols = [col for col in df.columns if col.startswith('Company_')]

company_means = {}

for col in company_cols:
    company_name = col.split('_')[1]
    mean_price = df[df[col] == 1]['Price'].mean()
    company_means[company_name] = mean_price

# Convert to series and sort
company_means = pd.Series(company_means).sort_values(ascending=False)

# Plotting the bar chart
plt.figure(figsize=(10, 6))
company_means.plot(kind='barh', color='skyblue')
plt.xlabel('Average Laptop Price')
plt.ylabel('Company')
plt.title('Average Laptop Price by Company')
plt.gca().invert_yaxis()  # To display highest price on top
plt.tight_layout()
plt.show()

# 4. How well does the model perform on laptops with high-end specifications compared to budget laptops?
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Define high-end and budget segments
high_end = df[df['Price'] > 100000]
budget = df[df['Price'] < 40000]

# Predict and evaluate
for segment, name in zip([high_end, budget], ['High-end', 'Budget']):
    X_seg = segment.drop('Price', axis=1)
    y_seg = segment['Price']
    y_pred_seg = rf.predict(X_seg)
    print(f"{name} Laptops R²:", r2_score(y_seg, y_pred_seg))
    print(f"{name} RMSE:", np.sqrt(mean_squared_error(y_seg, y_pred_seg)))

# 4. How well does the model perform on laptops with high-end specifications compared to budget laptops?
import matplotlib.pyplot as plt

r2_scores = [r2_score(high_end['Price'], rf.predict(high_end.drop('Price', axis=1))),
             r2_score(budget['Price'], rf.predict(budget.drop('Price', axis=1)))]

rmse_values = [np.sqrt(mean_squared_error(high_end['Price'], rf.predict(high_end.drop('Price', axis=1)))),
               np.sqrt(mean_squared_error(budget['Price'], rf.predict(budget.drop('Price', axis=1))))]

labels = ['High-end', 'Budget']

plt.figure(figsize=(10, 4))

# R² Plot
plt.subplot(1, 2, 1)
plt.bar(labels, r2_scores, color='skyblue')
plt.title("R² Score Comparison")
plt.ylabel("R² Score")

# RMSE Plot
plt.subplot(1, 2, 2)
plt.bar(labels, rmse_values, color='salmon')
plt.title("RMSE Comparison")
plt.ylabel("RMSE (₹)")

plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Visualize price distribution to identify outliers
sns.boxplot(x=df['Price'])
plt.title("Laptop Price Distribution with Outliers")
plt.show()

# Show feature importances from Random Forest
import pandas as pd
import matplotlib.pyplot as plt

feat_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head(15)

feat_importance.plot(kind='barh', title="Top 15 Important Features")
plt.xlabel("Feature Importance Score")
plt.show()

# 6. How does the model perform when predicting the prices of newly released laptops not present in the training dataset?
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Simulate "new releases" by taking the last 20 rows
new_laptops = df.tail(20)

# Drop from original training dataset
df_train_sim = df.iloc[:-20]

# Redefine training set
X_train_sim = df_train_sim.drop('Price', axis=1)
y_train_sim = df_train_sim['Price']

# Train new model
from sklearn.ensemble import RandomForestRegressor
rf_sim = RandomForestRegressor(random_state=42)
rf_sim.fit(X_train_sim, y_train_sim)

# Prepare new data
X_new = new_laptops.drop('Price', axis=1)
y_new = new_laptops['Price']

# Predict and evaluate
y_pred_new = rf_sim.predict(X_new)

print("Simulated 'New Laptop' Prediction Performance:")
print("R² Score:", r2_score(y_new, y_pred_new))
print("RMSE:", np.sqrt(mean_squared_error(y_new, y_pred_new)))

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(range(len(y_new)), y_new, color='blue', label='Actual Price', marker='o')
plt.scatter(range(len(y_pred_new)), y_pred_new, color='red', label='Predicted Price', marker='x')
plt.title("Actual vs Predicted Prices for New Laptops")
plt.xlabel("Laptop Index")
plt.ylabel("Price")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

residuals = y_new - y_pred_new

plt.figure(figsize=(8, 6))
plt.bar(range(len(residuals)), residuals, color='purple')
plt.axhline(0, color='black', linestyle='--')
plt.title("Residuals for New Laptop Predictions")
plt.xlabel("Laptop Index")
plt.ylabel("Prediction Error (Actual - Predicted)")
plt.grid(True)
plt.tight_layout()
plt.show()

comparison_df = pd.DataFrame({
    'Actual Price': y_new.values,
    'Predicted Price': y_pred_new.astype(int),
    'Error': (y_new - y_pred_new).astype(int)
})
print(comparison_df)